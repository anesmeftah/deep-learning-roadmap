{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "78794836"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "9a7512a7"
   },
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IyEgIJn3Yit",
    "outputId": "66b1d6fe-6c88-4e92-db43-9583eb3a8d71"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/anesmeftah/deep-learning-roadmap.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "88a04abc"
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.ImageFolder(root='/content/deep-learning-roadmap/Paper Implementation/ViT/test/' , transform=transform)\n",
    "testset = torchvision.datasets.ImageFolder(root='/content/deep-learning-roadmap/Paper Implementation/ViT/test/' , transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7072fc8b",
    "outputId": "63bc4255-97e6-43a5-8413-5ca18a285fda"
   },
   "outputs": [],
   "source": [
    "trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f99fded",
    "outputId": "05871066-6a74-4131-8895-e77f296c49cd"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(trainset , 32 , shuffle= True)\n",
    "testloader = DataLoader(testset , 32 , shuffle= True)\n",
    "trainloader , testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "528021cc",
    "outputId": "99d1d2ce-da27-4a0e-e140-7e7394436ede"
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(trainloader))\n",
    "\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "\n",
    "# View the batch shapes\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "4ef6c085",
    "outputId": "936dacfc-e865-4198-8ec6-70043a88d81e"
   },
   "outputs": [],
   "source": [
    "# Plot image with matplotlib\n",
    "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
    "plt.title(trainset.classes[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3a2d5d2",
    "outputId": "dbe02438-aeca-4449-c4de-3a724645d564"
   },
   "outputs": [],
   "source": [
    "# Create example values\n",
    "height = 224 # H (\"The training resolution is 224.\")\n",
    "width = 224 # W\n",
    "color_channels = 3 # C\n",
    "patch_size = 16 # P\n",
    "\n",
    "# Calculate N (number of patches)\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJIJpNg93urV",
    "outputId": "cb78c050-183f-440e-84ab-016f0a2c9e0c"
   },
   "outputs": [],
   "source": [
    "# Input shape (this is the size of a single image)\n",
    "embedding_layer_input_shape = (height, width, color_channels)\n",
    "\n",
    "# Output shape\n",
    "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
    "\n",
    "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
    "print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "hZPs_OH339I3"
   },
   "outputs": [],
   "source": [
    "# View single image function\n",
    "def view_image(trainset , image , label):\n",
    "  plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
    "  plt.title(trainset.classes[label])\n",
    "  plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "xXyUa3J0hgPK",
    "outputId": "28781cfb-ea12-416b-cf35-db6b6c60ad56"
   },
   "outputs": [],
   "source": [
    "view_image(trainset , image , label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "id": "ENjelVH-4kjP",
    "outputId": "113a7c68-2a9d-498e-9921-8ca023bed139"
   },
   "outputs": [],
   "source": [
    "# Change image shape to be compatible with matplotlib (color_channels, height, width) -> (height, width, color_channels)\n",
    "image_permuted = image.permute(1, 2, 0)\n",
    "\n",
    "# Index to plot the top row of patched pixels\n",
    "patch_size = 16\n",
    "plt.figure(figsize=(patch_size, patch_size))\n",
    "plt.imshow(image_permuted[:patch_size, :, :]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Q2MEv-Q84-4n",
    "outputId": "a3df5326-d984-481b-ac43-1541d1f44892"
   },
   "outputs": [],
   "source": [
    "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "num_patches = img_size/patch_size\n",
    "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "print(f\"Number of patches per row: {num_patches}\\\n",
    "        \\nNumber of patches per column: {num_patches}\\\n",
    "        \\nTotal patches: {num_patches*num_patches}\\\n",
    "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "# Create a series of subplots\n",
    "fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n",
    "                        ncols=img_size // patch_size,\n",
    "                        figsize=(num_patches, num_patches),\n",
    "                        sharex=True,\n",
    "                        sharey=True)\n",
    "\n",
    "# Loop through height and width of image\n",
    "for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n",
    "    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n",
    "\n",
    "        # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels))\n",
    "        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height\n",
    "                                        patch_width:patch_width+patch_size, # iterate through width\n",
    "                                        :]) # get all color channels\n",
    "\n",
    "        # Set up label information, remove the ticks for clarity and set labels to outside\n",
    "        axs[i, j].set_ylabel(i+1,\n",
    "                             rotation=\"horizontal\",\n",
    "                             horizontalalignment=\"right\",\n",
    "                             verticalalignment=\"center\")\n",
    "        axs[i, j].set_xlabel(j+1)\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "        axs[i, j].label_outer()\n",
    "\n",
    "# Set a super title\n",
    "fig.suptitle(f\"{trainset.classes[label]} -> Patchified\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "asWN4iQe5XZF"
   },
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "#Create the Conv2d layer\n",
    "\n",
    "conv2d = nn.Conv2d(in_channels= 3,\n",
    "                   out_channels= 768,\n",
    "                   kernel_size= patch_size,\n",
    "                   stride= patch_size,\n",
    "                   padding= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "AGonES5mTOxZ",
    "outputId": "5e7c22f7-4006-4f04-fbd6-60d528e60c76"
   },
   "outputs": [],
   "source": [
    "# View single image\n",
    "\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(trainset.classes[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vk80ZrsGX-Kg",
    "outputId": "9981a84d-c716-466a-f848-34cc85b2c844"
   },
   "outputs": [],
   "source": [
    "print(image.shape)\n",
    "print(image.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMGbaQ2JXhYP",
    "outputId": "b78083de-13c0-4235-9819-37ae9b6138da"
   },
   "outputs": [],
   "source": [
    "img_out_conv = conv2d(image.unsqueeze(0))\n",
    "print(img_out_conv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "AjDWcm98Yk8S"
   },
   "source": [
    "Visualize five random feature maps to see what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "THBEhZpuYNAu",
    "outputId": "12c55b57-275f-489f-a9f3-5c8df42cb6a4"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random_indexes = random.sample(range(0,768) , k=5) #pick 5 numbers\n",
    "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
    "\n",
    "fig , axs = plt.subplots(nrows = 1 , ncols = 5 , figsize=(12,12))\n",
    "for i, idx in enumerate(random_indexes):\n",
    "  img_conv_feature_map = img_out_conv[:, idx, : , :]\n",
    "  axs[i].imshow(img_conv_feature_map.squeeze().detach().numpy())\n",
    "  axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "cagmyYBvcHpR"
   },
   "source": [
    "Check out the numerical form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8Zeu16Ubyrs",
    "outputId": "278b5fa6-21a3-4db3-d80a-f16fa564c8ae"
   },
   "outputs": [],
   "source": [
    "#Get a single feature map in tensor form\n",
    "\n",
    "single_feature_map = img_out_conv[:, 0, :, :]\n",
    "single_feature_map , single_feature_map.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "2p2QUndTdjkt"
   },
   "source": [
    "# Flattening the patch embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "nBtoMQOHgP9d"
   },
   "source": [
    "Reading back the ViT paper it says we don't want to flatten the whole tensor, we only want to flatten the spatial dimension of the feature map which is in our case the `feature_map_height` and the `feature_map_width`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "rRM2Bk7Nch_6"
   },
   "outputs": [],
   "source": [
    "#Create flatten layer\n",
    "flatten = nn.Flatten(start_dim = 2, #height\n",
    "                     end_dim = 3) #width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "JIJuZ7b0g_z4",
    "outputId": "ea08b448-610c-476c-8fea-f42908b619eb"
   },
   "outputs": [],
   "source": [
    "# 1. View single image\n",
    "view_image(trainset , image , label)\n",
    "print(\"Original image shape : \" , image.shape)\n",
    "\n",
    "# 2. Turn image into feature maps\n",
    "img_out_conv = conv2d(image.unsqueeze(0))\n",
    "print(\"image feature map shape\" , img_out_conv.shape)\n",
    "\n",
    "# 3. Flatten the feature maps\n",
    "img_out_conv_flattened = flatten(img_out_conv)\n",
    "print(\"Flattened image feature map shape\" , img_out_conv_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBKDiMEfh-zE",
    "outputId": "39153e2a-30fd-4de6-8c8a-a613e8f04cc8"
   },
   "outputs": [],
   "source": [
    "# Get flattened image patch embeddings in right shape\n",
    "img_out_conv_flattened_reshaped = img_out_conv_flattened.permute(0, 2, 1)\n",
    "print(f\"Patch embedding sequence shape: {img_out_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "3oXkgX8UXuhe"
   },
   "source": [
    "We visualize one of the flattened feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "id": "yVucH09kjPfT",
    "outputId": "8c9dd333-7cf1-483c-f950-91849c73f2e4"
   },
   "outputs": [],
   "source": [
    "# Get a single flattened feature map\n",
    "single_flattened_feature_map = img_out_conv_flattened_reshaped[:, :, 0]\n",
    "\n",
    "# Plot the flattened feature map\n",
    "plt.figure(figsize=(22,22))\n",
    "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
    "plt.title(f\"Flattened feature map shape {single_flattened_feature_map.shape}\")\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "q5qG1ngDatV4"
   },
   "source": [
    "\n",
    "\n",
    "> The original Transformer architecture was designed to work with text. The Vision Transformer architecture (ViT) had the goal of using the original Transformer for images. This is why we're taking a 2D image and formatting it so it appears as a 1D sequence of text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "pbNSwJ2kbCAc"
   },
   "source": [
    "# Turning the ViT patch embedding layer into a PyTorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "id": "ZZXe1fJ1Zika"
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module): # Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
    "\n",
    "  def __init__(self,\n",
    "               in_channels = 3,\n",
    "               patch_size = 16,\n",
    "               embedding_dim = 768):\n",
    "    super().__init__()\n",
    "\n",
    "    self.patcher = nn.Conv2d(in_channels= in_channels,\n",
    "                             out_channels = embedding_dim,\n",
    "                             kernel_size = patch_size,\n",
    "                             stride = patch_size,\n",
    "                             padding = 0)\n",
    "\n",
    "    self.flatten = nn.Flatten(start_dim= 2 , end_dim= 3)\n",
    "\n",
    "  # The forward method\n",
    "  def forward(self , x):\n",
    "    image_resolution = x.shape[-1]\n",
    "    assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
    "\n",
    "    x_patched = self.patcher(x)\n",
    "    x_flattened = self.flatten(x_patched)\n",
    "\n",
    "    return x_flattened.permute(0, 2, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8WITiP9efAt",
    "outputId": "132822ee-ed48-45cc-b1f0-1d78d01bc48b"
   },
   "outputs": [],
   "source": [
    "# Create an instance of patch embedding layer\n",
    "patchify = PatchEmbedding(in_channels=3,\n",
    "                          patch_size=16,\n",
    "                          embedding_dim=768)\n",
    "\n",
    "# Pass a single image through\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "54r0SqCxfRpo"
   },
   "source": [
    "# Create the class token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uawYT_3ueswK",
    "outputId": "9aa90290-78cb-46db-dd68-cab568f257bf"
   },
   "outputs": [],
   "source": [
    "# View the patch embedding and patch embedding shape\n",
    "print(patch_embedded_image)\n",
    "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDx3Amn7hkg9",
    "outputId": "ec77ccd9-b194-4f8a-a98c-5797649dd4ce"
   },
   "outputs": [],
   "source": [
    "# Get the batch size and embedding dimension\n",
    "batch_size = patch_embedded_image.shape[0]\n",
    "embedding_dimension = patch_embedded_image.shape[-1]\n",
    "\n",
    "class_token = nn.Parameter(torch.ones(batch_size , 1 , embedding_dimension),\n",
    "                           requires_grad= True)\n",
    "\n",
    "print(class_token[:, :, :10])\n",
    "print(f\"Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embedding_dimension]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "id": "UZ_82qPRjf-R"
   },
   "outputs": [],
   "source": [
    "patch_embedded_image_with_class_embedding = torch.cat((class_token , patch_embedded_image) ,\n",
    "                                                     dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "PPfuSYdvUuu9"
   },
   "source": [
    "# Create the position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SWm3hURhq5Nx",
    "outputId": "5c743c93-c1eb-4293-bef4-9a4f57add28c"
   },
   "outputs": [],
   "source": [
    "# View the sequence of patch embedding with the prepended class embedding\n",
    "patch_embedded_image_with_class_embedding , patch_embedded_image_with_class_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "tfUbeyTyXuU9"
   },
   "outputs": [],
   "source": [
    "# Calculate N : number of patches\n",
    "number_patches = int((height * width) / patch_size**2)\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n",
    "\n",
    "# Create the learnable 1D position embedding\n",
    "position_embedding = nn.Parameter(torch.ones(1 , number_patches + 1 , embedding_dimension) , requires_grad= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "JVM4A4bVapEc"
   },
   "source": [
    "Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yr7wQAqvajtm",
    "outputId": "4b1cb5d4-e9d4-474a-aba9-820ca32bfb14"
   },
   "outputs": [],
   "source": [
    "print(position_embedding[:, :10, :10])\n",
    "print(f\"Position embedding shape: {position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxrMOW0FbjDm",
    "outputId": "500bf383-3002-4b30-c6b9-79874e36c64c"
   },
   "outputs": [],
   "source": [
    "# Add the position embedding to the patch and class token embedding\n",
    "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
    "print(patch_and_position_embedding)\n",
    "print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8hDqvMxb58l",
    "outputId": "a3e116fc-88a8-4efa-9384-034084264b33"
   },
   "outputs": [],
   "source": [
    "# 1. Set patch size\n",
    "patch_size = 16\n",
    "\n",
    "# 2. Print shape of original image tensor and get the image dimensions\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[1], image.shape[2]\n",
    "\n",
    "# 3. Get image tensor and add batch dimension\n",
    "x = image.unsqueeze(0)\n",
    "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
    "\n",
    "# 4. Create patch embedding layer\n",
    "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
    "                                       patch_size=patch_size,\n",
    "                                       embedding_dim=768)\n",
    "\n",
    "# 5. Pass image through patch embedding layer\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "# 6. Create class token embedding\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad=True)\n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "# 7. Prepend class token embedding to patch embedding\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
    "\n",
    "# 8. Create position embedding\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
    "                                  requires_grad=True)\n",
    "\n",
    "# 9. Add position embedding to patch embedding with class token\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "UhpflFU4mMCA"
   },
   "source": [
    "# Multi-Head Attention (MSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "id": "cxYDPpMZlXKW"
   },
   "outputs": [],
   "source": [
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               embedding_dim = 768,\n",
    "               num_heads = 12,\n",
    "               attn_dropout = 0):\n",
    "    super().__init__()\n",
    "\n",
    "    self.layer_norm = nn.LayerNorm(normalized_shape = embedding_dim)\n",
    "\n",
    "    self.multihead_attn = nn.MultiheadAttention(embed_dim= embedding_dim ,\n",
    "                                                num_heads= num_heads,\n",
    "                                                dropout=attn_dropout,\n",
    "                                                batch_first= True)\n",
    "  def forward(self , x):\n",
    "    x = self.layer_norm(x)\n",
    "    attn_output , _ = self.multihead_attn(query = x,\n",
    "                                            key = x,\n",
    "                                            value = x,\n",
    "                                            need_weights = False)\n",
    "    return attn_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "I0fCinQFs86s"
   },
   "source": [
    "We create an instance of our `MultiheadSelfAttentionBlock` and passing through the `patch_and_position_embedding` variable we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bnoq7z8Sscu4",
    "outputId": "a4587dc6-8238-4f81-aa28-a32a5f5fdb14"
   },
   "outputs": [],
   "source": [
    "# Create an instance of MSABlock\n",
    "multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1\n",
    "                                                             num_heads=12) # from Table 1\n",
    "\n",
    "# Pass patch and position image embedding through MSABlock\n",
    "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
    "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
    "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "4HFAEmRXuWJe"
   },
   "source": [
    "# Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "id": "s_8vINgCsyOF"
   },
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "  def __init__(self ,\n",
    "               embedding_dim = 768,\n",
    "               mlp_size = 3072,\n",
    "               dropout = 0.1):\n",
    "    super().__init__()\n",
    "    self.layer_norm = nn.LayerNorm(normalized_shape= embedding_dim)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(in_features= embedding_dim,\n",
    "                  out_features= mlp_size),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(p = dropout),\n",
    "        nn.Linear(in_features= mlp_size,\n",
    "                  out_features= embedding_dim),\n",
    "        nn.Dropout(p = dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self , x):\n",
    "    x = self.layer_norm(x)\n",
    "    x = self.mlp(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {
    "id": "rXCNxWHTzXje"
   },
   "source": [
    "We create an instance of our `MLPBlock` and passing through the `patched_image_through_msa_block`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUIHRq4pzR6S",
    "outputId": "f0fdf3a0-8c56-47e1-d30b-91884241e008"
   },
   "outputs": [],
   "source": [
    "# Create an instance of MLPBlock\n",
    "mlp_block = MLPBlock(embedding_dim=768,\n",
    "                     mlp_size=3072,\n",
    "                     dropout=0.1)\n",
    "\n",
    "# Pass output of MSABlock through MLPBlock\n",
    "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
    "print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\n",
    "print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "id": "rpiN6npPzthy"
   },
   "source": [
    "# Create the Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "id": "lK5ikk1Szl_h"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "  def __init__(self,\n",
    "               embedding_dim=768,\n",
    "               num_heads=12,\n",
    "               mlp_size=3072,\n",
    "               mlp_dropout=0.1,\n",
    "               attn_dropout=0):\n",
    "    super().__init__()\n",
    "    self.msa_block = MultiheadSelfAttentionBlock(embedding_dim= embedding_dim,\n",
    "                                                 num_heads= num_heads,\n",
    "                                                 attn_dropout= attn_dropout)\n",
    "\n",
    "    self.mlp_block = MLPBlock(embedding_dim= embedding_dim,\n",
    "                              mlp_size= mlp_size,\n",
    "                              dropout = mlp_dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.msa_block(x) + x\n",
    "    x = self.mlp_block(x) + x\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "id": "QLaOrmuq7D-E"
   },
   "source": [
    "# Create ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "id": "qX8frNKC30xi"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "  def __init__(self,\n",
    "               img_size = 224,\n",
    "               in_channels = 3,\n",
    "               patch_size = 16,\n",
    "               num_transformer_layers = 12, # layers from Table 1 for ViT-Base\n",
    "               embedding_dim = 768,\n",
    "               mlp_size = 3072,\n",
    "               num_heads = 12,\n",
    "               attn_dropout = 0,\n",
    "               mlp_dropout = 0.1,\n",
    "               embedding_dropout = 0.1,\n",
    "               num_classes = 1000 # default for ImageNet\n",
    "               ):\n",
    "    super().__init__()\n",
    "    assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "\n",
    "    self.num_patches = (img_size ** 2) // patch_size ** 2\n",
    "    self.class_embedding = nn.Parameter(data = torch.randn(1, 1, embedding_dim),\n",
    "                                        requires_grad = True)\n",
    "\n",
    "    self.position_embedding = nn.Parameter(data = torch.randn(1, self.num_patches + 1, embedding_dim),\n",
    "                                           requires_grad= True)\n",
    "\n",
    "    self.embedding_dropout = nn.Dropout(p = embedding_dropout)\n",
    "\n",
    "    self.patch_embedding = PatchEmbedding(in_channels = in_channels,\n",
    "                                          patch_size = patch_size,\n",
    "                                          embedding_dim= embedding_dim)\n",
    "\n",
    "    self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim= embedding_dim,\n",
    "                                                                       num_heads = num_heads,\n",
    "                                                                       mlp_size = mlp_size,\n",
    "                                                                       mlp_dropout= mlp_dropout)for _ in range(num_transformer_layers)])\n",
    "\n",
    "    self.classifier = nn.Sequential(\n",
    "        nn.LayerNorm(normalized_shape= embedding_dim),\n",
    "        nn.Linear(in_features= embedding_dim,\n",
    "                  out_features= num_classes)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    class_token = self.class_embedding.expand(batch_size, -1 ,-1)\n",
    "\n",
    "    x = self.patch_embedding(x)\n",
    "    x = torch.cat((class_token, x) , dim = 1)\n",
    "    x = self.position_embedding + x\n",
    "    x = self.embedding_dropout(x)\n",
    "    x = self.transformer_encoder(x)\n",
    "    x = self.classifier(x[: , 0])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ze0vvyE_Ighi",
    "outputId": "86e1bb4b-719b-4dd4-8c52-fd598fe5d428"
   },
   "outputs": [],
   "source": [
    "# Create a random tensor with same shape as a single image\n",
    "random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n",
    "\n",
    "# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
    "vit = ViT(num_classes=len(trainset.classes[label]))\n",
    "\n",
    "# Pass the random image tensor to our ViT instance\n",
    "vit(random_image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "id": "jE5uePF3Lj0u"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "id": "8VwjkehUMTo7"
   },
   "source": [
    "Optimizer : We can see they choose to use \"Adam\" optimizer\n",
    "Loss function : we are going to use CrossEntropyLoss because we are working with multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "id": "Md3etmXYLrD7"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = vit.parameters(),\n",
    "                             lr = 0.003,\n",
    "                             betas= (0.9 , 0.999),\n",
    "                             weight_decay = 0.3)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "id": "Xvu4PWLPNYYH"
   },
   "outputs": [],
   "source": [
    "# train a model\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model,\n",
    "          train_dataloader,\n",
    "          test_dataloader,\n",
    "          optimizer,\n",
    "          loss_fn,\n",
    "          epochs,\n",
    "          device = \"cuda\"):\n",
    "\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        for X, y in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\"):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            train_correct += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc = train_correct / len(train_dataloader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss, test_correct = 0, 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for X, y in tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Testing]\"):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_pred = model(X)\n",
    "\n",
    "                test_loss += loss_fn(y_pred, y).item()\n",
    "                test_correct += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc = test_correct / len(test_dataloader.dataset)\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831,
     "referenced_widgets": [
      "b0847d11ebac468e9fee2fa26b43aa8d",
      "75337057fd85471da5327c9587214466",
      "58d162cb461c4714a624322f5ae81a31",
      "48bc5c16384848deb829b6a4841a8683",
      "23dbf27a2b77470cb639ded5499b3afe",
      "2b0155f0a63d43f28742b93a4be27113",
      "a3d2f814815f4211a41d582014c465b2",
      "bf1cd2c163db4b0d8fa2bb6a45fbb33c",
      "268cfbd0f5c24fd889890499e796974f",
      "1525beebfce649cfa6f8e925330854e8",
      "0443172aae914401b9b4bc3569f57e15",
      "a133c3347b014597ae3bcf1a7daa4eab",
      "343e38880c284002975aa7835c0a1303",
      "dc4078a5cab045d9b3beae440c75bbce",
      "2e9bfbfde12947ccb0b0139ea14e4a5a",
      "3728fc7834d84bbda46812bbe43e0832",
      "4e71cd36fd31484188c8a6381c0ee6ff",
      "d08da8bde33b43b39e65e6d128669bbd",
      "553e2c58cf20465986efd86866998d90",
      "1113c341ea9049caabe3b290865e0edc",
      "ffff269f70e843a38f1dad4f74846b99",
      "feaec41e01a048fab6c36c218318dd46",
      "dc16fe8ac0a14aedacfb53c2532000fe",
      "b783c985af344169942ffe2254c1eb5f",
      "88b3effc8ba24e0db2b4777515b14840",
      "ed28ec04d7a14fb4960d5ebdd7b5938b",
      "87b78b49046244e39f12655b8df00d53",
      "f1b8f866eef44f41ab4ce809c1b4d934",
      "aba39307885b4fab9719c9ea7488a7b0",
      "159e09db8dc94ca9a70f7167d3a4d90f",
      "5cd23c6c8a074eee8a8d182fbc368d52",
      "8c333e513f64492ba349cbd18b04ac6f",
      "97756e34c18644debd9c0a038e090c9c",
      "9e2ffeee78214cbd90364e445c5af0f2",
      "719608fd7bee425e8b5da4ab8ba6653e",
      "3b803d5ed5324981b8622aa89fc7edeb",
      "6c4541e925c9400cbcf2421409423c57",
      "4d522fc97fc44f88a9cbaacf210c4938",
      "54c14cd494d44c088cf3c3a3ef37a136",
      "25411e2e708a4057893f5f78960231fd",
      "2fe4cd2158804a4f9f4029605e11ee75",
      "1b2745e1e3284096b7f389a67b78adff",
      "5c8467b65462429788852a625436f4c2",
      "5c07affd2eaf4bc5804a326bf8fd762f",
      "0c95064f9a444985ad1f4bd8a845629e",
      "5094bdc6c6854e3da6dd4bea3f431caa",
      "69e15fa30a3644b3b351096319625988",
      "51831c11e78a42029b7e3415559a21b4",
      "8263aa1df9944e7da88fbe2a32142818",
      "c30d183e72374aa4a71e66621e6a75be",
      "138d367c122c4b729f9d2556f27d1385",
      "1feb58e22fe34b23b9aa52948028d87f",
      "b3e835228b74448aa0a80612ca93811b",
      "d55d818edea347798fc96bcd9390e817",
      "5d68ee3670324903a6742eafeb4a3bb6",
      "cc830b8bf2aa4cfb9b56ce2b68b6caa5",
      "5f91f6bd1dea45c3a124b630273beb33",
      "c6d00d2aa32148efa1e7c3b0397018c6",
      "c0f95820fbd24d738f14cb53748c1751",
      "19d82e6fce254728a72377b6a59fadba",
      "796a79a4cb2944439ab13c4e1b640d59",
      "00688b4533604205abaa5713ecc09003",
      "116e20d58f3549c8b4048f94ce06a3a0",
      "04321622a74546f68bef14ec158c7beb",
      "c1676bbda10e45189e2c597f908c84b9",
      "ebcfa8eeeb2e4f2cbcccda2f5fdbcc8f",
      "e724092a62444a939cf68dc77c71e616",
      "336ac6e353fa4d4581b6ccf1472c6abb",
      "7f2f12443b95420b951fc5b74a118795",
      "bca142ecb1f34929ac32ed01e2d7b701",
      "ff86ae1c2603453ca69f164231994e65",
      "ea678a8e203e437582b69ab52c92cedd",
      "67c99fe9f9c346ebacbef579eb266926",
      "1c9bd3262a4647a99568bb5b9409a764",
      "314174e882024c859e7a8edd7016f5fc",
      "b5a971977a3f4c68b4d902c0e3b43118",
      "ef1efa1af1b14dce8cc22564ac411947",
      "c803fcafe02847c3b38f93a7cbf4f3e4",
      "391201ab2efd4a6484937ae7617bd18d",
      "1b9532478b77478a8a37df987bbb4e86",
      "00166513fd614754a1b5082d6a545bf2",
      "c6be30873d474cafb2efde6ea59f6688",
      "485669f09e954223a6608d4f511569fe",
      "20e446c24a67405d9c75ecf2017dda35",
      "cc7338cff3fd40a6987872cdd9f6719e",
      "cd99e1a1dc5a4ca9a41896dd17513094",
      "5ad00b36b7934d7ab70eefa273903fb0",
      "8a8e7015ddc34850a3bec121505d118c",
      "6a742da0f9cf4146bfd8a98fc75e301a",
      "4deea943c17442a6aca0b58e8798ffa0",
      "e8a3199820f149e8bd80e6a562806675",
      "b8ed8b66d9b24ad2919d871971a06470",
      "4e47468dd43f438ead2df1bcf31de541",
      "673b4bdc966f4fc0afabe437a5d554dd",
      "990c657c67c74da6bd2fb69f51589bfa",
      "9c9a5df959bf4653aa0c6a35e387fb21",
      "9ffbff4295714b5c96d26a141fdacf49",
      "db135360a7e9479eb4124fd6b92c5073",
      "e67b458664e64fe0a151e16e7fc48e9f",
      "c5e4e363b3ce432a8615998d9d5d0fc9",
      "9bf3b964b8174c6788e5760ed7c9c044",
      "c3395a229bb349d0bb62f96624f57c7e",
      "08d48eff9d0c40b782df0da9849d0321",
      "daef086f516d4e99a7d5818ecf45eabc",
      "454ca825395446539cdb4fcacfe2be47",
      "f13d2f00f05048f6a4573c7448f44b79",
      "e7d4994ffbbc4ae593e3a564dc944359",
      "1a672571b5044604a41dca52c5ba4db8",
      "c07738daf7b84a529a9fdedd75ffc042",
      "2c0b13af44d94d2fa129bff347a33ad1",
      "ec6f77da52ec463bb24aad85063e88ed",
      "05c65d5b85e4438ebbdc94083b6204d8",
      "c44682dab67345d4a7551cecc0a32b78",
      "31b440caa969445f80da7413b1d72778",
      "ae715a0295154ef0ad5a1a0bd50727c5",
      "1eebb60cf635471ca256ccb7cdbcb45d",
      "730cd2eab4d54b34854dd0d0f884d956",
      "16a7738f7f5b4158bc5f45f0315d8fbb",
      "1def4341bbf1431fbe0bd44f7302fb9d",
      "063e678321284dd18cd1ded863b30358",
      "477061c0ea3241f2bc05b176281d872c",
      "c55cbd5f0be04f2fb74d8732ec56b35f",
      "3a64424004fe48ad8a2fe6f77af4245a",
      "f7faa741021e4186b35af40b7bf9ea5b",
      "d86e05eba40640fe9523ec913ec43f3c",
      "32521758b9e1430286f8aeba6fa0c752",
      "d91eb30ab49c4764bcf36b4162daaa29",
      "f6624f81b6a345639df5e34fc09c61ed",
      "22a83df301ea421582d6822d06e7ea50",
      "d93e7018835d463b9d8d7f3f4eadccc5",
      "493b8d83dc304dedb3b97cffc3fd92d4",
      "368e0ef9ba19404b871c269116ea4a7d",
      "8931a7405ebe45959ca03001fbc724e6",
      "633f1f2499f147b79242e12ee027e960",
      "8f0669d9857245c6857f15fbe4ef8931",
      "2974568857824d0bafb50ab70bea1415",
      "939a587967c44029a47eb66d6dcec50c",
      "81a6f2ddea8b4d48b7216d639a2adf27",
      "a84c540079fb425680753d51b100031a",
      "ded66fe347a74dc9a8613dc93d187523",
      "67cf37f4c10f42d996f03e367d07b39e",
      "480f8a0e089e4b60b38ca488c7c2ebb1",
      "37f8a8ad48524d5cbfcb200dcf9a5d87",
      "725f3554f9cb4131861b5658d870054b",
      "41e9b57cb80d4a6ebd5d494cff31f2f1",
      "09c031aed4a24e1d8b75b9d9e0468c2f",
      "fff6dbd8da2a472191b5202d9526036d",
      "a630a53e557c4abdad7dae1f5845332c",
      "aae41fd9bbeb42459d50d02a54641517",
      "b56f54d465a2417b8c6d4f96c1ed42d0",
      "b113a1faf9e14133bcf5d76e48c3ee6b",
      "d4987e2a507f4b2399be7173319b2b3e",
      "d15f1d4af3294ceaa5d307800479fc50",
      "65a5090608fe41fa806e61510154dfe2",
      "63316a496207439a84477c6871e14931",
      "fa877a9e769f4760b0d0240e3670dd84",
      "cb68edd7cfe64dbd99f7e9d181a2bda7",
      "364ebe97854e4963ba9cfcf6038703f2",
      "348596e7b1394ae28aa8d1bc7b8c3f49",
      "7b907cc90b224b60a6abef242c7699da",
      "c281535b1a0b4f82b0c9f57700d1bf83",
      "aab7487d4cf1473b8eff75c8ed1433b2",
      "45674f67fe244f83bb94122a073ba5fd",
      "8cfc28862d5842f681e33e64b32548d0",
      "1fe25f921c744b07a51cf0d512bb1b87",
      "8e28ca132e144a05be5f22bd424f14d3",
      "47f848a1ef8541449a2391b0120c8c89",
      "3a31aad3d2a34863926c98da54cc3bb8",
      "da59f2ecb31949508092e413b61a590e",
      "e136fe52c5bb42048bd671406e15957f",
      "10ca0cbaf4194e718bc257475fca6fef",
      "0b7744f9fe7442a898b44910d27c907b",
      "3f5aff6eb47449f8a298ebdca456032c",
      "ddf15a3bda9f44ed8f12dc401fdf34f7",
      "984fdff043334a05a9896cfb9740c8a3",
      "64065de6559f49a3bd4190439513e07d",
      "7cba4e6fa19b433b9706ac9313984dad",
      "bbd8185d6e664e7ca467c9436a389110",
      "0f628da2e224443bbe857cd27997be36",
      "523830d7ade842c592bfe59a529b6000",
      "1a16b57e74fc4e448e769a49eb518826",
      "92fe594e2183484c995b595481827135",
      "0eecaaccec1c464fac694ac16dd25d05",
      "8cb85428f244452b9e24b4e352023d87",
      "600675eae5204abaaa7ef7bf3f558327",
      "a41a1586557e4dd7b5a239eda49b14d4",
      "1964b69e872647cf997786f17077559b",
      "78a4fe0f48cd42cd8c6cc615d4bda5e3",
      "992d17dae65a4a2a9aa3eaa806f88c24",
      "a995c7438817498db67c230dde232ae5",
      "8ce6a97aa9214683910c4c4f04331531",
      "8f3560233c584a3e8a4685ec19cbeb93",
      "05383471d4e748bebb848935aa7b9eac",
      "0829d4dff49e4d2e883358526a3564bf",
      "734c2aa3529a4cc5858b82f00deb1f90",
      "c37b627caaab4a09af9068e17e461e53",
      "a894fb0ee2594746adca7a26bf83b5b0",
      "e75bc6f0545d473ab3c970a2044a658d",
      "a114b81ca7ab4563a1b4ae2bd93b3623",
      "8bd35d5ddf744491ae6e09f58c236d7b",
      "4d2c2f7a73a141309f5c292adb136a1c",
      "eec214e2b192480397efb826467a9c04",
      "5920a1563940417094a806811fe38657",
      "4ff9855733024f968ad6ed69d64ef192",
      "1a9b4a831c3e4c488a3ff7893974a7ad",
      "3188a4dfdc33439aa6ea00d6809b9e4c",
      "e3a5facfa8ec4df198ae4953fe18aaa3",
      "96a9808737674b25a7d885987c49137a",
      "3353f9c08d924f379d8c8702f41dae16",
      "35a14399577e4001802f8f8cc07a872c",
      "dc2d5f8e4b68474ebbf5a8c27e9827d3",
      "3a6e9f11aca84173a48d7ad49c789a37",
      "8b225c87ca714da09cd2bd83cd65ecc6",
      "49e9e772f495485ab51d0e5455802fa4",
      "3904fd2e29c84fb7b8024054c904702b",
      "ef924730630f4a14b911718f6b3b62df",
      "2a97f2c4d3684d7e8a8e67e7a0c26491",
      "6ce6fd1bd7264baf9408e3384009df58",
      "bde760f3d936443b94a3c9c520253dee",
      "32433329dd2245f6985b054a293b0059"
     ]
    },
    "id": "ODEAr2q-Oawr",
    "outputId": "4cfa18c2-419e-40b5-807f-e0fa9f09d2d6"
   },
   "outputs": [],
   "source": [
    "results = train(vit ,\n",
    "                train_dataloader= trainloader,\n",
    "                test_dataloader= testloader,\n",
    "                optimizer= optimizer,\n",
    "                epochs = 10,\n",
    "                loss_fn = loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "skRrQlQ0O0ZB",
    "outputId": "111de73b-d9dc-4539-89b2-d67333a74af7"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(results[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, results[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(epochs, results[\"test_loss\"], label=\"Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, results[\"train_acc\"], label=\"Train Accuracy\")\n",
    "plt.plot(epochs, results[\"test_acc\"], label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "id": "ZBPseyW6PsVa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
